<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="个人博客" type="application/atom+xml" />






<meta name="description" content="有困难就是一个字&quot;干&quot;">
<meta property="og:type" content="website">
<meta property="og:title" content="个人博客">
<meta property="og:url" content="https://lightnine/github.io/page/4/index.html">
<meta property="og:site_name" content="个人博客">
<meta property="og:description" content="有困难就是一个字&quot;干&quot;">
<meta property="og:locale">
<meta property="article:author" content="liang">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://lightnine/github.io/page/4/"/>





  <title>个人博客</title>
  








<meta name="generator" content="Hexo 7.0.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lightnine/github.io/docker%E4%B8%8Etensorflow%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/docker%E4%B8%8Etensorflow%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8.html" itemprop="url">docker与tensorflow结合使用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-10T22:03:08+08:00">
                2018-08-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近这段时间一直在学习docker的使用,以及如何在docker中使用tensorflow.今天就把在docker中如何使用tensorflow记录一下.</p>
<h1 id="docker安装"><a href="#docker安装" class="headerlink" title="docker安装"></a>docker安装</h1><p>我是把docker安装在centos 7.4操作系统上面,在vmware中装的centos,vmware中安装centos很简单.具体的网络配置可以参考<a href="https://segmentfault.com/a/1190000008743806">vmware nat配置</a>.docker安装很简单,找到docker官网,直接按照上面的步骤安装即可.运行<code>docker version</code>查看版本如下</p>
<img src="/docker%E4%B8%8Etensorflow%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8/docker-version.png" class="">
<p>因为docker 采用的是客户端/服务端的结构,所以这里可以看到client以及server,它们分别都有版本号.</p>
<h1 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h1><p>在docker中运行tensorflow的第一步就是要找到自己需要的镜像,我们可以去<a href="https://hub.docker.com">docker hub</a>找到自己需要的tensorflow镜像.tensorflow的镜像主要分两类,一种是在CPU上面跑的,还有一种是在GPU上面跑的,如果需要GPU的,那么还需要安装<strong>nvidia-docker</strong>.这里我使用的是CPU版本的.当然我们还需要选择具体的tensorflow版本.这里我拉取的命令如下:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull tensorflow/tensorflow:1.9.0-devel-py3</span><br></pre></td></tr></table></figure>
<p>拉取成功之后,运行<code>docker images</code>可以看到有tensorflow镜像.</p>
<h1 id="tensorflow在docker中使用"><a href="#tensorflow在docker中使用" class="headerlink" title="tensorflow在docker中使用"></a>tensorflow在docker中使用</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -p 8888:8888 --name tf-1.9 tensorflow/tensorflow:1.9.0-devel-py3</span><br></pre></td></tr></table></figure>
<p>运行上面的命令,在容器中启动镜像.<code>-p</code>表示指定端口映射,即将本机的8888端口映射到容器的8888端口.<code>--name</code>用来指定容器的名字为<code>tf-1.9</code>.因为这里采用的镜像是devel模式的,所以默认不启动jupyter.如果想使用默认启动jupyter的镜像,那么直接拉取不带devel的镜像就可以.即拉取最近的镜像<code>docker pull tensorflow/tensorflow</code><br>启动之后,我们就进入了容器,<code>ls /</code> 查看容器根目录内容,可以看到有<code>run_jupyter.sh</code>文件.运行此文件,即在根目录下执行<code>./run_jupyter.sh --allow-root</code>,<code>--allow-root</code>参数是因为jupyter启动不推荐使用root,这里是主动允许使用root.然后在浏览器中就可以访问jupyter的内容了.<br><img src="/docker%E4%B8%8Etensorflow%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8/jupyter.png" class=""></p>
<h1 id="创建自己的镜像"><a href="#创建自己的镜像" class="headerlink" title="创建自己的镜像"></a>创建自己的镜像</h1><p>上面仅仅是跑了一个什么都没有的镜像,如果我们需要在镜像里面跑我们的深度学习程序怎么办呢?这首先做的第一步就是要制作我们自己的镜像.这里我们跑一个简单的mnist数据集,程序可以直接去tensorflow上面找一个例子程序.这里我的程序如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment"># you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"># You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment"># ==============================================================================</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;Simple, end-to-end, LeNet-5-like convolutional MNIST model example.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This should achieve a test error of 0.7%. Please keep this model as simple and</span></span><br><span class="line"><span class="string">linear as possible, it is meant as a tutorial for simple convolutional models.</span></span><br><span class="line"><span class="string">Run with --self_test on the command line to execute a short self-test.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> xrange  <span class="comment"># pylint: disable=redefined-builtin</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># CVDF mirror of http://yann.lecun.com/exdb/mnist/</span></span><br><span class="line"><span class="comment"># 如果WORK_DIRECTORY中没有需要的数据,则从此地址下载数据</span></span><br><span class="line">SOURCE_URL = <span class="string">&#x27;https://storage.googleapis.com/cvdf-datasets/mnist/&#x27;</span></span><br><span class="line"><span class="comment"># 训练数据位置</span></span><br><span class="line"><span class="comment"># WORK_DIRECTORY = &#x27;data&#x27;</span></span><br><span class="line">WORK_DIRECTORY = <span class="string">&#x27;./MNIST-data&#x27;</span></span><br><span class="line">IMAGE_SIZE = <span class="number">28</span></span><br><span class="line">NUM_CHANNELS = <span class="number">1</span></span><br><span class="line">PIXEL_DEPTH = <span class="number">255</span></span><br><span class="line">NUM_LABELS = <span class="number">10</span></span><br><span class="line">VALIDATION_SIZE = <span class="number">5000</span>  <span class="comment"># Size of the validation set.</span></span><br><span class="line">SEED = <span class="number">66478</span>  <span class="comment"># Set to None for random seed.</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">NUM_EPOCHS = <span class="number">10</span></span><br><span class="line">EVAL_BATCH_SIZE = <span class="number">64</span></span><br><span class="line">EVAL_FREQUENCY = <span class="number">100</span>  <span class="comment"># Number of steps between evaluations.</span></span><br><span class="line"></span><br><span class="line">FLAGS = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印信息设置</span></span><br><span class="line"><span class="comment"># logging.basicConfig(format=&#x27;%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s&#x27;,</span></span><br><span class="line"><span class="comment">#                     level=logging.DEBUG)</span></span><br><span class="line">logging.basicConfig(level=logging.DEBUG,  <span class="comment"># 控制台打印的日志级别</span></span><br><span class="line">                    filename=<span class="string">&#x27;cnn_mnist.log&#x27;</span>,</span><br><span class="line">                    filemode=<span class="string">&#x27;a&#x27;</span>,  <span class="comment"># 模式，有w和a，w就是写模式，每次都会重新写日志，覆盖之前的日志</span></span><br><span class="line">                    <span class="comment"># a是追加模式，默认如果不写的话，就是追加模式</span></span><br><span class="line">                    <span class="built_in">format</span>=</span><br><span class="line">                    <span class="string">&#x27;%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s&#x27;</span></span><br><span class="line">                    <span class="comment"># 日志格式</span></span><br><span class="line">                    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_type</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return the type of the activations, weights, and placeholder variables.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> FLAGS.use_fp16:</span><br><span class="line">        <span class="keyword">return</span> tf.float16</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> tf.float32</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">maybe_download</span>(<span class="params">filename</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Download the data from Yann&#x27;s website, unless it&#x27;s already here.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(WORK_DIRECTORY):</span><br><span class="line">        tf.gfile.MakeDirs(WORK_DIRECTORY)</span><br><span class="line">    filepath = os.path.join(WORK_DIRECTORY, filename)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(filepath):</span><br><span class="line">        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)</span><br><span class="line">        <span class="keyword">with</span> tf.gfile.GFile(filepath) <span class="keyword">as</span> f:</span><br><span class="line">            size = f.size()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Successfully downloaded&#x27;</span>, filename, size, <span class="string">&#x27;bytes.&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> filepath</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_data</span>(<span class="params">filename, num_images</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Extract the images into a 4D tensor [image index, y, x, channels].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Values are rescaled from [0, 255] down to [-0.5, 0.5].</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    logging.info(<span class="string">&#x27;Extracting&#x27;</span> + filename)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Extracting&#x27;</span>, filename)</span><br><span class="line">    <span class="keyword">with</span> gzip.<span class="built_in">open</span>(filename) <span class="keyword">as</span> bytestream:</span><br><span class="line">        bytestream.read(<span class="number">16</span>)</span><br><span class="line">        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)</span><br><span class="line">        data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)</span><br><span class="line">        data = (data - (PIXEL_DEPTH / <span class="number">2.0</span>)) / PIXEL_DEPTH</span><br><span class="line">        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_labels</span>(<span class="params">filename, num_images</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Extract the labels into a vector of int64 label IDs.&quot;&quot;&quot;</span></span><br><span class="line">    logging.info(<span class="string">&#x27;Extracting&#x27;</span> + filename)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Extracting&#x27;</span>, filename)</span><br><span class="line">    <span class="keyword">with</span> gzip.<span class="built_in">open</span>(filename) <span class="keyword">as</span> bytestream:</span><br><span class="line">        bytestream.read(<span class="number">8</span>)</span><br><span class="line">        buf = bytestream.read(<span class="number">1</span> * num_images)</span><br><span class="line">        labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)</span><br><span class="line">    <span class="keyword">return</span> labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fake_data</span>(<span class="params">num_images</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Generate a fake dataset that matches the dimensions of MNIST.&quot;&quot;&quot;</span></span><br><span class="line">    data = numpy.ndarray(</span><br><span class="line">        shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS),</span><br><span class="line">        dtype=numpy.float32)</span><br><span class="line">    labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)</span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> xrange(num_images):</span><br><span class="line">        label = image % <span class="number">2</span></span><br><span class="line">        data[image, :, :, <span class="number">0</span>] = label - <span class="number">0.5</span></span><br><span class="line">        labels[image] = label</span><br><span class="line">    <span class="keyword">return</span> data, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">error_rate</span>(<span class="params">predictions, labels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return the error rate based on dense predictions and sparse labels.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">100.0</span> - (</span><br><span class="line">            <span class="number">100.0</span> *</span><br><span class="line">            numpy.<span class="built_in">sum</span>(numpy.argmax(predictions, <span class="number">1</span>) == labels) /</span><br><span class="line">            predictions.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">_</span>):</span><br><span class="line">    <span class="keyword">if</span> FLAGS.self_test:</span><br><span class="line">        logging.info(<span class="string">&#x27;Running self-test.&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Running self-test.&#x27;</span>)</span><br><span class="line">        train_data, train_labels = fake_data(<span class="number">256</span>)</span><br><span class="line">        validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)</span><br><span class="line">        test_data, test_labels = fake_data(EVAL_BATCH_SIZE)</span><br><span class="line">        num_epochs = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Get the data.</span></span><br><span class="line">        train_data_filename = maybe_download(<span class="string">&#x27;train-images-idx3-ubyte.gz&#x27;</span>)</span><br><span class="line">        train_labels_filename = maybe_download(<span class="string">&#x27;train-labels-idx1-ubyte.gz&#x27;</span>)</span><br><span class="line">        test_data_filename = maybe_download(<span class="string">&#x27;t10k-images-idx3-ubyte.gz&#x27;</span>)</span><br><span class="line">        test_labels_filename = maybe_download(<span class="string">&#x27;t10k-labels-idx1-ubyte.gz&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Extract it into numpy arrays.</span></span><br><span class="line">        train_data = extract_data(train_data_filename, <span class="number">60000</span>)</span><br><span class="line">        train_labels = extract_labels(train_labels_filename, <span class="number">60000</span>)</span><br><span class="line">        test_data = extract_data(test_data_filename, <span class="number">10000</span>)</span><br><span class="line">        test_labels = extract_labels(test_labels_filename, <span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate a validation set.</span></span><br><span class="line">        validation_data = train_data[:VALIDATION_SIZE, ...]</span><br><span class="line">        validation_labels = train_labels[:VALIDATION_SIZE]</span><br><span class="line">        train_data = train_data[VALIDATION_SIZE:, ...]</span><br><span class="line">        train_labels = train_labels[VALIDATION_SIZE:]</span><br><span class="line">        num_epochs = NUM_EPOCHS</span><br><span class="line">    train_size = train_labels.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This is where training samples and labels are fed to the graph.</span></span><br><span class="line">    <span class="comment"># These placeholder nodes will be fed a batch of training data at each</span></span><br><span class="line">    <span class="comment"># training step using the &#123;feed_dict&#125; argument to the Run() call below.</span></span><br><span class="line">    train_data_node = tf.placeholder(</span><br><span class="line">        data_type(),</span><br><span class="line">        shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))</span><br><span class="line">    train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))</span><br><span class="line">    eval_data = tf.placeholder(</span><br><span class="line">        data_type(),</span><br><span class="line">        shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The variables below hold all the trainable weights. They are passed an</span></span><br><span class="line">    <span class="comment"># initial value which will be assigned when we call:</span></span><br><span class="line">    <span class="comment"># &#123;tf.global_variables_initializer().run()&#125;</span></span><br><span class="line">    conv1_weights = tf.Variable(</span><br><span class="line">        tf.truncated_normal([<span class="number">5</span>, <span class="number">5</span>, NUM_CHANNELS, <span class="number">32</span>],  <span class="comment"># 5x5 filter, depth 32.</span></span><br><span class="line">                            stddev=<span class="number">0.1</span>,</span><br><span class="line">                            seed=SEED, dtype=data_type()))</span><br><span class="line">    conv1_biases = tf.Variable(tf.zeros([<span class="number">32</span>], dtype=data_type()))</span><br><span class="line">    conv2_weights = tf.Variable(tf.truncated_normal(</span><br><span class="line">        [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>], stddev=<span class="number">0.1</span>,</span><br><span class="line">        seed=SEED, dtype=data_type()))</span><br><span class="line">    conv2_biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">64</span>], dtype=data_type()))</span><br><span class="line">    fc1_weights = tf.Variable(  <span class="comment"># fully connected, depth 512.</span></span><br><span class="line">        tf.truncated_normal([IMAGE_SIZE // <span class="number">4</span> * IMAGE_SIZE // <span class="number">4</span> * <span class="number">64</span>, <span class="number">512</span>],</span><br><span class="line">                            stddev=<span class="number">0.1</span>,</span><br><span class="line">                            seed=SEED,</span><br><span class="line">                            dtype=data_type()))</span><br><span class="line">    fc1_biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">512</span>], dtype=data_type()))</span><br><span class="line">    fc2_weights = tf.Variable(tf.truncated_normal([<span class="number">512</span>, NUM_LABELS],</span><br><span class="line">                                                  stddev=<span class="number">0.1</span>,</span><br><span class="line">                                                  seed=SEED,</span><br><span class="line">                                                  dtype=data_type()))</span><br><span class="line">    fc2_biases = tf.Variable(tf.constant(</span><br><span class="line">        <span class="number">0.1</span>, shape=[NUM_LABELS], dtype=data_type()))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We will replicate the model structure for the training subgraph, as well</span></span><br><span class="line">    <span class="comment"># as the evaluation subgraphs, while sharing the trainable parameters.</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">data, train=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;The Model definition.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 2D convolution, with &#x27;SAME&#x27; padding (i.e. the output feature map has</span></span><br><span class="line">        <span class="comment"># the same size as the input). Note that &#123;strides&#125; is a 4D array whose</span></span><br><span class="line">        <span class="comment"># shape matches the data layout: [image index, y, x, depth].</span></span><br><span class="line">        conv = tf.nn.conv2d(data,</span><br><span class="line">                            conv1_weights,</span><br><span class="line">                            strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                            padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">        <span class="comment"># Bias and rectified linear non-linearity.</span></span><br><span class="line">        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))</span><br><span class="line">        <span class="comment"># Max pooling. The kernel size spec &#123;ksize&#125; also follows the layout of</span></span><br><span class="line">        <span class="comment"># the data. Here we have a pooling window of 2, and a stride of 2.</span></span><br><span class="line">        pool = tf.nn.max_pool(relu,</span><br><span class="line">                              ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                              strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                              padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">        conv = tf.nn.conv2d(pool,</span><br><span class="line">                            conv2_weights,</span><br><span class="line">                            strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                            padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))</span><br><span class="line">        pool = tf.nn.max_pool(relu,</span><br><span class="line">                              ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                              strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                              padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">        <span class="comment"># Reshape the feature map cuboid into a 2D matrix to feed it to the</span></span><br><span class="line">        <span class="comment"># fully connected layers.</span></span><br><span class="line">        pool_shape = pool.get_shape().as_list()</span><br><span class="line">        reshape = tf.reshape(</span><br><span class="line">            pool,</span><br><span class="line">            [pool_shape[<span class="number">0</span>], pool_shape[<span class="number">1</span>] * pool_shape[<span class="number">2</span>] * pool_shape[<span class="number">3</span>]])</span><br><span class="line">        <span class="comment"># Fully connected layer. Note that the &#x27;+&#x27; operation automatically</span></span><br><span class="line">        <span class="comment"># broadcasts the biases.</span></span><br><span class="line">        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)</span><br><span class="line">        <span class="comment"># Add a 50% dropout during training only. Dropout also scales</span></span><br><span class="line">        <span class="comment"># activations such that no rescaling is needed at evaluation time.</span></span><br><span class="line">        <span class="keyword">if</span> train:</span><br><span class="line">            hidden = tf.nn.dropout(hidden, <span class="number">0.5</span>, seed=SEED)</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(hidden, fc2_weights) + fc2_biases</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training computation: logits + cross-entropy loss.</span></span><br><span class="line">    logits = model(train_data_node, <span class="literal">True</span>)</span><br><span class="line">    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        labels=train_labels_node, logits=logits))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># L2 regularization for the fully connected parameters.</span></span><br><span class="line">    regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +</span><br><span class="line">                    tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))</span><br><span class="line">    <span class="comment"># Add the regularization term to the loss.</span></span><br><span class="line">    loss += <span class="number">5e-4</span> * regularizers</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optimizer: set up a variable that&#x27;s incremented once per batch and</span></span><br><span class="line">    <span class="comment"># controls the learning rate decay.</span></span><br><span class="line">    batch = tf.Variable(<span class="number">0</span>, dtype=data_type())</span><br><span class="line">    <span class="comment"># Decay once per epoch, using an exponential schedule starting at 0.01.</span></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        <span class="number">0.01</span>,  <span class="comment"># Base learning rate.</span></span><br><span class="line">        batch * BATCH_SIZE,  <span class="comment"># Current index into the dataset.</span></span><br><span class="line">        train_size,  <span class="comment"># Decay step.</span></span><br><span class="line">        <span class="number">0.95</span>,  <span class="comment"># Decay rate.</span></span><br><span class="line">        staircase=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># Use simple momentum for the optimization.</span></span><br><span class="line">    optimizer = tf.train.MomentumOptimizer(learning_rate,</span><br><span class="line">                                           <span class="number">0.9</span>).minimize(loss,</span><br><span class="line">                                                         global_step=batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predictions for the current training minibatch.</span></span><br><span class="line">    train_prediction = tf.nn.softmax(logits)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predictions for the test and validation, which we&#x27;ll compute less often.</span></span><br><span class="line">    eval_prediction = tf.nn.softmax(model(eval_data))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Small utility function to evaluate a dataset by feeding batches of data to</span></span><br><span class="line">    <span class="comment"># &#123;eval_data&#125; and pulling the results from &#123;eval_predictions&#125;.</span></span><br><span class="line">    <span class="comment"># Saves memory and enables this to run on smaller GPUs.</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">eval_in_batches</span>(<span class="params">data, sess</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Get all predictions for a dataset by running it in small batches.&quot;&quot;&quot;</span></span><br><span class="line">        size = data.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> size &lt; EVAL_BATCH_SIZE:</span><br><span class="line">            logging.error(<span class="string">&quot;batch size for evals larger than dataset: %d&quot;</span> % size)</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;batch size for evals larger than dataset: %d&quot;</span> % size)</span><br><span class="line">        predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)</span><br><span class="line">        <span class="keyword">for</span> begin <span class="keyword">in</span> xrange(<span class="number">0</span>, size, EVAL_BATCH_SIZE):</span><br><span class="line">            end = begin + EVAL_BATCH_SIZE</span><br><span class="line">            <span class="keyword">if</span> end &lt;= size:</span><br><span class="line">                predictions[begin:end, :] = sess.run(</span><br><span class="line">                    eval_prediction,</span><br><span class="line">                    feed_dict=&#123;eval_data: data[begin:end, ...]&#125;)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                batch_predictions = sess.run(</span><br><span class="line">                    eval_prediction,</span><br><span class="line">                    feed_dict=&#123;eval_data: data[-EVAL_BATCH_SIZE:, ...]&#125;)</span><br><span class="line">                predictions[begin:, :] = batch_predictions[begin - size:, :]</span><br><span class="line">        <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a local session to run the training.</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># Run all the initializers to prepare the trainable parameters.</span></span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        logging.info(<span class="string">&#x27;Initialized!&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Initialized!&#x27;</span>)</span><br><span class="line">        <span class="comment"># Loop through training steps.</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> xrange(<span class="built_in">int</span>(num_epochs * train_size) // BATCH_SIZE):</span><br><span class="line">            <span class="comment"># Compute the offset of the current minibatch in the data.</span></span><br><span class="line">            <span class="comment"># Note that we could use better randomization across epochs.</span></span><br><span class="line">            offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)</span><br><span class="line">            batch_data = train_data[offset:(offset + BATCH_SIZE), ...]</span><br><span class="line">            batch_labels = train_labels[offset:(offset + BATCH_SIZE)]</span><br><span class="line">            <span class="comment"># This dictionary maps the batch data (as a numpy array) to the</span></span><br><span class="line">            <span class="comment"># node in the graph it should be fed to.</span></span><br><span class="line">            feed_dict = &#123;train_data_node: batch_data,</span><br><span class="line">                         train_labels_node: batch_labels&#125;</span><br><span class="line">            <span class="comment"># Run the optimizer to update weights.</span></span><br><span class="line">            sess.run(optimizer, feed_dict=feed_dict)</span><br><span class="line">            <span class="comment"># print some extra information once reach the evaluation frequency</span></span><br><span class="line">            <span class="keyword">if</span> step % EVAL_FREQUENCY == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># fetch some extra nodes&#x27; data</span></span><br><span class="line">                l, lr, predictions = sess.run([loss, learning_rate, train_prediction],</span><br><span class="line">                                              feed_dict=feed_dict)</span><br><span class="line">                elapsed_time = time.time() - start_time</span><br><span class="line">                start_time = time.time()</span><br><span class="line">                logging.info(<span class="string">&#x27;Step %d (epoch %.2f), %.1f ms&#x27;</span> %(step, <span class="built_in">float</span>(step) * BATCH_SIZE / train_size, <span class="number">1000</span> * elapsed_time / EVAL_FREQUENCY))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Step %d (epoch %.2f), %.1f ms&#x27;</span> %</span><br><span class="line">                      (step, <span class="built_in">float</span>(step) * BATCH_SIZE / train_size,</span><br><span class="line">                       <span class="number">1000</span> * elapsed_time / EVAL_FREQUENCY))</span><br><span class="line">                logging.info(<span class="string">&#x27;Minibatch loss: %.3f, learning rate: %.6f&#x27;</span> % (l, lr))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Minibatch loss: %.3f, learning rate: %.6f&#x27;</span> % (l, lr))</span><br><span class="line">                logging.info(<span class="string">&#x27;Minibatch error: %.1f%%&#x27;</span> % error_rate(predictions, batch_labels))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Minibatch error: %.1f%%&#x27;</span> % error_rate(predictions, batch_labels))</span><br><span class="line">                logging.info(<span class="string">&#x27;Validation error: %.1f%%&#x27;</span> % error_rate(eval_in_batches(validation_data, sess), validation_labels))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Validation error: %.1f%%&#x27;</span> % error_rate(</span><br><span class="line">                    eval_in_batches(validation_data, sess), validation_labels))</span><br><span class="line">                sys.stdout.flush()</span><br><span class="line">        <span class="comment"># Finally print the result!</span></span><br><span class="line">        test_error = error_rate(eval_in_batches(test_data, sess), test_labels)</span><br><span class="line">        logging.info(<span class="string">&#x27;Test error: %.1f%%&#x27;</span> % test_error)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Test error: %.1f%%&#x27;</span> % test_error)</span><br><span class="line">        <span class="keyword">if</span> FLAGS.self_test:</span><br><span class="line">            logging.info(<span class="string">&#x27;test_error&#x27;</span> + test_error)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;test_error&#x27;</span>, test_error)</span><br><span class="line">            <span class="keyword">assert</span> test_error == <span class="number">0.0</span>, <span class="string">&#x27;expected 0.0 test_error, got %.2f&#x27;</span> % (</span><br><span class="line">                test_error,)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&#x27;--use_fp16&#x27;</span>,</span><br><span class="line">        default=<span class="literal">False</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&#x27;Use half floats instead of full floats if True.&#x27;</span>,</span><br><span class="line">        action=<span class="string">&#x27;store_true&#x27;</span>)</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&#x27;--self_test&#x27;</span>,</span><br><span class="line">        default=<span class="literal">False</span>,</span><br><span class="line">        action=<span class="string">&#x27;store_true&#x27;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&#x27;True if running a self test.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    FLAGS, unparsed = parser.parse_known_args()</span><br><span class="line">    tf.app.run(main=main, argv=[sys.argv[<span class="number">0</span>]] + unparsed)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里我在原来的程序基础上面稍微改了下,因为我已经提前将数据下载好了,所以我让程序直接读取本机指定目录下的训练数据,同时增加了日志文件输出.这是为了在公司的容器云平台上测试获取容器输出文件</p>
<h2 id="编写Dockerfile"><a href="#编写Dockerfile" class="headerlink" title="编写Dockerfile"></a>编写Dockerfile</h2><p>我们可以在我们的用户目录下,创建一个空的文件夹,将mnist数据集以及程序文件都拷贝进这个文件夹下.其实数据集应该是放在数据卷中,但是这里为了方便,我直接将训练数据打进了镜像中.然后创建Dockerfile,文件内容如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM tensorflow/tensorflow:1.9.0-devel-py3</span><br><span class="line"></span><br><span class="line">COPY . /home/ll</span><br><span class="line">WORKDIR /home/ll</span><br><span class="line">CMD [<span class="string">&#x27;python&#x27;</span>, <span class="string">&#x27;convolutional.py&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>即Dockerfile文件中最后一行表示容器启动的运行的命令</p>
<h2 id="build镜像"><a href="#build镜像" class="headerlink" title="build镜像"></a>build镜像</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t tf:1.9 .</span><br></pre></td></tr></table></figure>
<p><code>-t</code>参数指定镜像跟tag,最后的<code>.</code>指定了镜像中的上下文.构建完之后使用<code>docker images</code>可以查看多了<code>tf:1.9</code>镜像</p>
<h2 id="运行镜像"><a href="#运行镜像" class="headerlink" title="运行镜像"></a>运行镜像</h2><p>运行下面的命令,运行上一步构建好的镜像</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --name <span class="built_in">test</span> tf:1.9</span><br></pre></td></tr></table></figure>
<p>然后就能够看到训练的输出.<br><img src="/docker%E4%B8%8Etensorflow%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8/tensorflow.png" class=""><br>同时可以在看一个连接,进入容器,即运行下面命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it <span class="built_in">test</span> /bin/bash</span><br></pre></td></tr></table></figure>
<p>可以看到如下内容<br><img src="/docker%E4%B8%8Etensorflow%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8/container.png" class=""><br>即看到了cnn_mnist.log的日志输出文件</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lightnine/github.io/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html" itemprop="url">docker常用命令</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-31T16:53:41+08:00">
                2018-07-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="docker命令"><a href="#docker命令" class="headerlink" title="docker命令"></a>docker命令</h1><h2 id="docker基本命令"><a href="#docker基本命令" class="headerlink" title="docker基本命令"></a>docker基本命令</h2><h3 id="查看docker版本"><a href="#查看docker版本" class="headerlink" title="查看docker版本"></a>查看docker版本</h3><p><code>docker version</code></p>
<h3 id="查看docker信息"><a href="#查看docker信息" class="headerlink" title="查看docker信息"></a>查看docker信息</h3><p><code>docker info</code></p>
<h3 id="启动docker-服务"><a href="#启动docker-服务" class="headerlink" title="启动docker 服务"></a>启动docker 服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo service docker start</span><br><span class="line">或者</span><br><span class="line">sudo systemctl start docker</span><br></pre></td></tr></table></figure>
<h2 id="镜像命令"><a href="#镜像命令" class="headerlink" title="镜像命令"></a>镜像命令</h2><h3 id="列出本机的所有image文件"><a href="#列出本机的所有image文件" class="headerlink" title="列出本机的所有image文件"></a>列出本机的所有image文件</h3><p><code>docker image ls</code></p>
<h3 id="删除image文件"><a href="#删除image文件" class="headerlink" title="删除image文件"></a>删除image文件</h3><p><code>docker image rm [imageName]</code></p>
<h3 id="拉取image文件"><a href="#拉取image文件" class="headerlink" title="拉取image文件"></a>拉取image文件</h3><p><code>docker image pull library/hello-world</code><br>library/hello-world是 image 文件在仓库里面的位置，其中library是 image 文件所在的组，hello-world是 image 文件的名字.因为Docker官方提供的镜像都在library中,所以可以省略library,即使用下面的命令<br><code>docker image pull hello-world</code></p>
<h3 id="运行image-文件"><a href="#运行image-文件" class="headerlink" title="运行image 文件"></a>运行image 文件</h3><p><code>docker container run hello-world</code><br>run命令每运行一次,就会新建一个容器.docker container run命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。因此，前面的docker image pull命令并不是必需的步骤.<br><code>-v</code>参数: 用来进行数据卷的挂载,将本机主机的目录挂载到容器的某一目录</p>
<h2 id="容器命令"><a href="#容器命令" class="headerlink" title="容器命令"></a>容器命令</h2><h3 id="终止容器"><a href="#终止容器" class="headerlink" title="终止容器"></a>终止容器</h3><p><code>docker container kill [containID]</code></p>
<blockquote>
<p>image 文件生成的容器实例，本身也是一个文件，称为容器文件。也就是说，一旦容器生成，就会同时存在两个文件： image 文件和容器文件。而且关闭容器并不会删除容器文件，只是容器停止运行而已。</p>
</blockquote>
<h3 id="列出本机正在运行的容器"><a href="#列出本机正在运行的容器" class="headerlink" title="列出本机正在运行的容器"></a>列出本机正在运行的容器</h3><p><code>docker container ls</code></p>
<h3 id="列出本机所有容器，包括终止运行的容器"><a href="#列出本机所有容器，包括终止运行的容器" class="headerlink" title="列出本机所有容器，包括终止运行的容器"></a>列出本机所有容器，包括终止运行的容器</h3><p><code>docker container ls --all</code></p>
<h3 id="删除容器文件"><a href="#删除容器文件" class="headerlink" title="删除容器文件"></a>删除容器文件</h3><p><code>docker container rm [containerID]</code>或<br><code>docker rm [containerID]</code><br>注: 删除正在运行的容器,需要添加<code>-f</code>参数;<code>-v</code>参数可以删除没有用的数据卷</p>
<h3 id="查看容器信息"><a href="#查看容器信息" class="headerlink" title="查看容器信息"></a>查看容器信息</h3><p><code>docker ps</code></p>
<h3 id="重启容器"><a href="#重启容器" class="headerlink" title="重启容器"></a>重启容器</h3><p>restart命令会将运行的容器终止,然后在重新启动<br><code>docker container restart [containerID 或 name]</code></p>
<h3 id="导出容器"><a href="#导出容器" class="headerlink" title="导出容器"></a>导出容器</h3><p>导出容器快照到本地文件<br><code>docker export [containerID]</code><br>示例:<br><code>docker export 7691a814370e &gt; ubuntu.tar</code></p>
<h3 id="创建image-文件"><a href="#创建image-文件" class="headerlink" title="创建image 文件"></a>创建image 文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker image build -t koa-demo .</span><br><span class="line">或者</span><br><span class="line">docker image build -t koa-demo:0.0.1 .</span><br></pre></td></tr></table></figure>
<p><strong>-t</strong> 用来指定image 文件的名字,后面可以用冒号指定标签.如果不指定,默认的标签就是<strong>latest</strong>. 最后的点表示Dockerfile文件所在的路径,一个点表示当前路径</p>
<h3 id="从image文件生成容器"><a href="#从image文件生成容器" class="headerlink" title="从image文件生成容器"></a>从image文件生成容器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker container run -p 8000:3000 -it koa-demo /bin/bash</span><br><span class="line">或者</span><br><span class="line">docker container run -p 8000:3000 -it koa-demo:0.0.1 /bin/bash</span><br></pre></td></tr></table></figure>
<p>参数含义:</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">-p参数:</span> <span class="string">容器的3000端口映射到本机的8000端口.</span></span><br><span class="line"><span class="string">-it参数:</span> <span class="string">容器的Shell映射到当前的Shell,然后你在本机窗口输入的命令,就会传入容器.</span></span><br><span class="line"><span class="attr">koa-demo:0.0.1:</span> <span class="string">image文件的名字,默认标签是latest.</span></span><br><span class="line"><span class="string">/bin/bash:</span> <span class="string">容器启动以后,内部第一个执行的命令.这里启动Bash,保证用户可以使用Shell</span></span><br></pre></td></tr></table></figure>
<p><code>docker container run --rm -p 8000:3000 -it koa-demo /bin/bash</code><br>—rm参数:在容器终止运行后自动删除容器文件</p>
<h2 id="其他有用的命令"><a href="#其他有用的命令" class="headerlink" title="其他有用的命令"></a>其他有用的命令</h2><h3 id="docker-container-start"><a href="#docker-container-start" class="headerlink" title="docker container start"></a>docker container start</h3><p>前面的docker container run命令是新建容器，每运行一次，就会新建一个容器。同样的命令运行两次，就会生成两个一模一样的容器文件。如果希望重复使用容器，就要使用docker container start命令，它用来启动已经生成、已经停止运行的容器文件。<br><code>docker container start [containerID]</code></p>
<h3 id="docker-container-stop"><a href="#docker-container-stop" class="headerlink" title="docker container stop"></a>docker container stop</h3><p>前面的docker container kill命令终止容器运行，相当于向容器里面的主进程发出 SIGKILL 信号。而docker container stop命令也是用来终止容器运行，相当于向容器里面的主进程发出 SIGTERM 信号，然后过一段时间再发出 SIGKILL 信号.<br><code>docker container stop [containerID]</code><br>这两个信号的差别是，应用程序收到 SIGTERM 信号以后，可以自行进行收尾清理工作，但也可以不理会这个信号。如果收到 SIGKILL 信号，就会强行立即终止，那些正在进行中的操作会全部丢失。</p>
<h3 id="docker-container-logs"><a href="#docker-container-logs" class="headerlink" title="docker container logs"></a>docker container logs</h3><p>docker container logs命令用来查看 docker 容器的输出，即容器里面 Shell 的标准输出。如果docker run命令运行容器的时候，没有使用-it参数，就要用这个命令查看输出。<br><code>docker container logs [containerID]</code></p>
<h3 id="docker-container-exec"><a href="#docker-container-exec" class="headerlink" title="docker container exec"></a>docker container exec</h3><p>docker container exec命令用于进入一个正在运行的 docker 容器。如果docker run命令运行容器的时候，没有使用-it参数，就要用这个命令进入容器。一旦进入了容器，就可以在容器的 Shell 执行命令了。<br><code>docker container exec -it [containerID] /bin/bash</code></p>
<h3 id="docker-container-cp"><a href="#docker-container-cp" class="headerlink" title="docker container cp"></a>docker container cp</h3><p>docker container cp命令用于从正在运行的 Docker 容器里面，将文件拷贝到本机。下面是拷贝到当前目录的写法。<br><code>docker container cp [containID]:[/path/to/file] .</code></p>
<h3 id="删除所有不运行的容器"><a href="#删除所有不运行的容器" class="headerlink" title="删除所有不运行的容器"></a>删除所有不运行的容器</h3><p><code>docker rm $(docker ps -a -q)</code></p>
<p>参考文章:<a href="http://www.ruanyifeng.com/blog/2018/02/docker-tutorial.html">Docker 入门教程-阮一峰</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lightnine/github.io/Understanding-LSTM-Networks.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Understanding-LSTM-Networks.html" itemprop="url">Understanding LSTM Networks</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-17T11:22:56+08:00">
                2018-07-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这是一篇译文,<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">原文地址</a>.如果英文可以,建议直接看英文.</p>
<h1 id="循环神经网络-Recurrent-Neural-Networks"><a href="#循环神经网络-Recurrent-Neural-Networks" class="headerlink" title="循环神经网络(Recurrent Neural Networks)"></a>循环神经网络(Recurrent Neural Networks)</h1><p>在我们思考时,我们不会从头开始,肯定会在思考时加入之前的知识.就如同当你在阅读当前的博客时,你读的每个单词都是基于前面的单词.你不会扔掉所有的东西,然后在从头开始.你的想法有持久性.<br>传统的神经网络不能使用之前信息.假设你使用传统神经网络来将一部电影中的正在发生的事件分类,怎么使用当前事件之前的信息是很困难的.<br>循环神经网络解决了这个问题,在其中有循环,允许信息能够保持.<br><img src="/Understanding-LSTM-Networks/RNN-rolled.png" class="" title="Recurrent Neural Networks have loops."><br>这个图看起来有点奇怪,我们可以将其展开,如下图所示<br><img src="/Understanding-LSTM-Networks/RNN-unrolled.png" class="" title="An unrolled recurrent neural network"><br>从图中可以看出,RNN使用了前一时刻的状态来预测当前的状态<br>RNN在很多领域都取得了成功,比如:语音识别,自然语言处理,翻译,图像字幕等等.这里Andrej Karpathy的<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">blog</a>,里面列举了RNN一些应用.<br>本文主要介绍LSTM模型,LSTM模型是RNN的一种变种,在很多领域的表现都要比一般的RNN模型更好</p>
<h1 id="长期依赖存在的问题-The-Problem-of-Long-Term-Dependencies"><a href="#长期依赖存在的问题-The-Problem-of-Long-Term-Dependencies" class="headerlink" title="长期依赖存在的问题(The Problem of Long-Term Dependencies)"></a>长期依赖存在的问题(The Problem of Long-Term Dependencies)</h1><p>加入我们要基于之前的单词预测下一个单词,如”the clouds are in the <em>sky</em>“,我们要预测最后一个单词sky.在这个例子中,我们不需要更多的上下文,很明显最后一个单词是sky.RNN针对这种情况有很好的表现.<br><img src="/Understanding-LSTM-Networks/RNN-shorttermdepdencies.png" class=""><br>但是在有些情况下,我们需要更多的上下文.例如,我们要预测”I grew up in France… I speak fluent <em>French</em>.”中的最后一个单词French.从最近的信息,比如speak fluent等,可以推测最后一个单词是一种语言.但是如果要知道具体的语言,我们需要更多的上下文.预测点和其对应的相关信息之间的差距很大是由很大可能的.<br>但是,不幸的是随着差距增加,RNN不可能学会如何连接这种信息<br><img src="/Understanding-LSTM-Networks/RNN-longtermdependencies.png" class=""><br>从图中可以看出$h_{t+1}$不能很好地利用$X_0$和$X_1$的信息.理论上,RNN能够学习到长期依赖,但是在实践中,要想在RNN中使用这些长期依赖很困难.幸运的是LSTM模型解决了这个问题.</p>
<h1 id="LSTM网络-LSTM-Networks"><a href="#LSTM网络-LSTM-Networks" class="headerlink" title="LSTM网络(LSTM Networks)"></a>LSTM网络(LSTM Networks)</h1><p>LSTM全称Long Short Term Memory,长短期记忆网络.LSTM的设计就是为了避免长依赖问题,记忆长周期的信息是LSTM的默认行为.<br>所有的递归神经网络都具有重复模块的链式结构.在标准的RNN中,重复模块是一个非常简单的结构,例如单层tanh<br><img src="/Understanding-LSTM-Networks/LSTM3-SimpleRNN.png" class="" title="The repeating module in a standard RNN contains a single layer."><br>LSTM也有这种链式结构,但是重复的模块有一个不同的结构,在其中一个模块中,包含一个四层的神经网络,并且这四层以一种特别方式进行交互.<br><img src="/Understanding-LSTM-Networks/LSTM3-chain.png" class="" title="The repeating module in an LSTM contains four interacting layers."><br>图中的示例如下<br><img src="/Understanding-LSTM-Networks/LSTM2-notation.png" class=""><br>接下来我会详细介绍其中涉及的内容.</p>
<h1 id="LSTMs中的核心思想-The-Core-Idea-Behind-LSTMs"><a href="#LSTMs中的核心思想-The-Core-Idea-Behind-LSTMs" class="headerlink" title="LSTMs中的核心思想(The Core Idea Behind LSTMs)"></a>LSTMs中的核心思想(The Core Idea Behind LSTMs)</h1><p>LSTMs中的核心是单元状态,在图中就是最上面的那条水平线.单元状态就像是一条输送带.单元状态沿着整个链流动,对其只有一些线性作用,信息的流动很容易.<br><img src="/Understanding-LSTM-Networks/LSTM3-C-line.png" class=""><br>LSTM能够移除或者添加信息到单元状态,并且由成为门的结构来控制.门可以选择性的让信息通过或不通过,它是由sigmoid神经网络层和点积操作组成.<br><img src="/Understanding-LSTM-Networks/LSTM3-gate.png" class=""><br>sigmoid层的输出范围0-1,描述了可以通过多少的信息.输出为0意味着什么都不通过,输出为1意味着都能通过.从LSTM的结构中,我们可以看到LSTM的一个重复模块包括三个这样的门.</p>
<h1 id="一步一步认识LSTM"><a href="#一步一步认识LSTM" class="headerlink" title="一步一步认识LSTM"></a>一步一步认识LSTM</h1><p>LSTM的第一步就是决定从单元状态中丢弃哪些信息.这个决定是由一个称为”遗忘门(forget gate layer)”单层sigmoid层组成.它接收$h<em>{t-1}$和$x_t$作为输入,输出0到1之间的数字.对于前一个单元状态$C</em>{t-1}$,当遗忘门的输出为1时,表示完全保留$C<em>{t-1}$;当遗忘门的输出为0时,表示完全舍弃$C</em>{t-1}$.<br>我们在回到之前根据之前的内容来预测下一个单词这个问题.在这样的问题中,单元状态可能包括当前对象的性别,所以可以使用正确的代词.但是当遇到一个新的对象时,我们就想要忘记老对象的性别.<br><img src="/Understanding-LSTM-Networks/LSTM3-focus-f.png" class=""></p>
<blockquote>
<p>图中的$W<em>f \cdot [h</em>{t-1},x<em>t]$,大家看起来有些不清楚.个人认为比较合适的写法如下<br>$\sigma(W_fx_t + U_r h</em>{t-1} + b_f)$.大家可以参考下<a href="http://blog.echen.me/2017/05/30/exploring-lstms/">英文博客</a>,<a href="https://www.jiqizhixin.com/articles/2017-07-24-2">中文博客</a></p>
</blockquote>
<p>接下来这一步就是决定新信息中的哪些部分需要保存在单元状态中.这包括两个部分,第一部分是sigmoid层(input gate layer),决定更新的值.第二部分是一个tanh层,用来创建一个新的候选值.在下一步中,我们将会把这两个结合起来用来给单元状态做更新.在语言模型的例子中,这一步就代表我们将新对象的性别加入单元状态中,用来代替我们需要忘记的老对象性别.<br><img src="/Understanding-LSTM-Networks/LSTM3-focus-i.png" class=""><br>现在要把旧的单元状态$C_{t-1}$更新为$C_t$.上一步已经决定了更新的内容.我们把旧的单元状态乘以$f_t$,表示我们需要忘记的内容.然后将结果加上$i_t*\tilde{C_t}$,这表示将候选值进行缩放或者延伸.在语言模型的例子中,这表示我们从旧对象的性别中去除信息,然后加上新的信息.<br><img src="/Understanding-LSTM-Networks/LSTM3-focus-C.png" class=""><br>最终,我们需要决定输出的内容.输出内容基于单元状态,但是需要一些处理.首先,我们使用一个sigmoid层来决定单元状态的哪部分需要输出.然后,我们将单元状态经过tanh(将值限定在-1到1之间),在乘以sigmoid gate的输出.所以我们仅仅输出了我们决定的.<br>对于语言模型例子,因为网络之前看到了主语,所以接下来它可能想要输出的信息是关于动词的.例如,网络会根据主语的单数或复数来决定接下来动词的形式.<br><img src="/Understanding-LSTM-Networks/LSTM3-focus-o.png" class=""></p>
<h1 id="长短期记忆模型的其他形式-Variants-on-Long-Short-Term-Memory"><a href="#长短期记忆模型的其他形式-Variants-on-Long-Short-Term-Memory" class="headerlink" title="长短期记忆模型的其他形式(Variants on Long Short Term Memory)"></a>长短期记忆模型的其他形式(Variants on Long Short Term Memory)</h1><p>上面提到的是LSTM的一般形式.但是还有好多LSTM的一些变形,虽然变化不大.其中一个流行的LSTM变形是由<a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf">Gers &amp; Schmidhuber (2000)</a>提出.此模型添加了”peephole connections”,这意味着让门看到了单元状态.<br><img src="/Understanding-LSTM-Networks/LSTM3-var-peepholes.png" class=""><br>从图中可以看到模型对于所有的gate都添加了peepholes,但是有很多文章只添加了一些peepholes,而另一些则没有.</p>
<p>LSTM另一种变形是将input gate和forget gate进行耦合.这种网络不会单独的决定哪些内容需要忘记,哪些新的信息需要添加,而是一起做这些决定.当我们将要输入内容时,我们仅仅忘记;当我们忘记旧的内容时,我们仅仅输入一些新的值.模型的结构如下<br><img src="/Understanding-LSTM-Networks/LSTM3-var-tied.png" class=""></p>
<p>一种引入注意的LSTM变形是GRU(Gated Recurrent Unit),它将forget gate和input gate结合为一个单独的”update gate”.它同样融合了单元状态和隐藏状态以及一些其他的改变.GRU模型要比标准的LSTM模型简单,并且越来越流行.<br><img src="/Understanding-LSTM-Networks/LSTM3-var-GRU.png" class=""><br>这里仅仅列举了一些LSTM变形,还有很多其他的LSTM变形.比如Depth Gated RNNs by <a href="http://arxiv.org/pdf/1508.03790v2.pdf">Yao, et al. (2015)</a>.同时在解决长期依赖问题上,也有跟LSTM完全不同的方法,比如Clockwork RNNs by <a href="http://arxiv.org/pdf/1402.3511v1.pdf">Koutnik, et al. (2014)</a>.<br><a href="http://arxiv.org/pdf/1503.04069.pdf">Greff, et al. (2015)</a>对这些变形做了一个对比.<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz, et al. (2015)</a>测试很多的RNN结构,发现在一些特定的任务上比LSTM模型表现的更好的模型.</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">colah博客</a><br><a href="http://blog.echen.me/2017/05/30/exploring-lstms/">echen博客</a></p>
<blockquote>
<p>echen博客中公式介绍的较为详细</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lightnine/github.io/python3%E5%AD%A6%E4%B9%A0.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/python3%E5%AD%A6%E4%B9%A0.html" itemprop="url">python3学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-10T11:48:04+08:00">
                2018-07-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h1><p>在python中,一个py文件就是一个模块.而为了避免模块名称冲突,Python又引入了按目录来组织模块的方法,成为Package.引入了包后,对应的模块名前面就添加了包名.类似于Java中的Package.</p>
<p>__init__.py:在包目录下,必须有__init__.py这个文件,如果没有这个文件,那么python会将此文件作为一个普通目录,而不在是一个package.__init__.py可以为空,也可以有代码(一般都是一些import语句).__init__.py本身是一个模块,其模块名为对应的package名称.</p>
<h1 id="Python示例程序"><a href="#Python示例程序" class="headerlink" title="Python示例程序"></a>Python示例程序</h1><figure class="highlight python"><figcaption><span>hello.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27; a test module &#x27;</span></span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">&#x27;Michael Liao&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    args = sys.argv</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(args)==<span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Hello, world!&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">len</span>(args)==<span class="number">2</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Hello, %s!&#x27;</span> % args[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Too many arguments!&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    test()</span><br></pre></td></tr></table></figure>
<p>程序说明:程序开始的一,二行是标准注释.第一行注释说明这个文件可以直接在Unix/Linux/Max上运行.第二行表示文件本身使用utf-8编码.第四行是模块的文档注释,第六行是作者信息.<br><code>if __name__ == &#39;__main__&#39;:</code>:当我们在命令行运行模块时,python解释器会将一个特殊的变量<code>__name__</code>设置为<code>__main__</code>.但是如果在其他模块中导入此模块,python解释器并不是设置<code>__name__</code>变量.</p>
<h1 id="python中的作用域"><a href="#python中的作用域" class="headerlink" title="python中的作用域"></a>python中的作用域</h1><p>正常的变量或函数是公开的,可以直接引用如<code>abc</code>,<code>xyz</code>等.<br>类似<code>__xxx__</code>的变量时特殊变量,也可以直接引用,但是有特殊用途.如<code>__author__</code>,<code>__name__</code>.而模块的文档注释可以用<code>__doc__</code>来访问<br>类似于<code>_x</code>,<code>__x</code>这样定义的变量或函数是非公开的,类似Java的private.</p>
<h1 id="python模块搜索路径"><a href="#python模块搜索路径" class="headerlink" title="python模块搜索路径"></a>python模块搜索路径</h1><p>当我们引入模块时,即使用import导入模块时,python解释器从搜索路径中查找对应的模块.如果找不到,则会报错.<br>默认情况下,Python解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在sys模块的path变量中.下面代码可以查看搜索路径内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import sys</span><br><span class="line">&gt;&gt;&gt; sys.path</span><br></pre></td></tr></table></figure>
<h2 id="修改搜索路径"><a href="#修改搜索路径" class="headerlink" title="修改搜索路径"></a>修改搜索路径</h2><p>可以使用两种方式修改搜索路径</p>
<ol>
<li>直接修改<code>sys.path</code> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import sys</span><br><span class="line">&gt;&gt;&gt; sys.path.append(<span class="string">&#x27;待添加路径&#x27;</span>)</span><br></pre></td></tr></table></figure>
 但是这种方式只在程序运行时有效,程序运行结束后失效</li>
<li>设置<code>PATHONPATH</code><br> 直接将待添加的路径加入<code>PATHONPATH</code>环境变量中即可</li>
</ol>
<h1 id="类与对象"><a href="#类与对象" class="headerlink" title="类与对象"></a>类与对象</h1><p>在python类中,<code>__init__</code>方法就相当于Java中类的构造函数,<strong>self</strong>表示实例自身.<br>如果类中的变量定义前加了两个下划线,那么此变量就成为一个私有变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, score</span>):</span><br><span class="line">        self.__name = name</span><br><span class="line">        self.score = score</span><br><span class="line">a = Student(<span class="string">&#x27;1&#x27;</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(a.__name)</span><br><span class="line"><span class="built_in">print</span>(a.score)</span><br></pre></td></tr></table></figure>
<p>打印<code>__name</code>时,将会报错,而可以打印score的内容.</p>
<h2 id="鸭子类型"><a href="#鸭子类型" class="headerlink" title="鸭子类型"></a>鸭子类型</h2><blockquote>
<p>注明:个人解决因为无法限制方法入参的类型,那么传递进去什么都可以</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Animal</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Animal is running...&#x27;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_twice</span>(<span class="params">animal</span>):</span><br><span class="line">    animal.run()</span><br><span class="line">    animal.run()</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Timer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Start...&#x27;</span>)</span><br><span class="line">run_twice(Timer())</span><br></pre></td></tr></table></figure>
<p>在Java中,如果定义<code>run_twice</code>方法,必须定义函数的形参类型,如果指定了只能接受Animal类型,则只能传递Animal类型或其子类.但是在python不是这样,只要传递进的类型中存在<code>run</code>方法即可.不过python中不存在限制入参是什么类型.</p>
<blockquote>
<p>这就是动态语言的“鸭子类型”，它并不要求严格的继承体系，一个对象只要“看起来像鸭子，走起路来像鸭子”，那它就可以被看做是鸭子。</p>
</blockquote>
<h2 id="slots"><a href="#slots" class="headerlink" title="__slots__"></a><code>__slots__</code></h2><p>python是动态语言，可以在运行过程中给类实例以及类本身添加属性和方法。如果想要限制动态添加的属性，则可以在类中定义<code>__slots__</code>,如下代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    __slots__ = (<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>) <span class="comment"># 运行绑定的属性限制为name和age</span></span><br></pre></td></tr></table></figure>
<h2 id="property"><a href="#property" class="headerlink" title="@property"></a><code>@property</code></h2><p><code>@property</code>是python内置装饰器,可以在给属性添加getter以及setter方法，从而简化代码。如下使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._score</span><br><span class="line"></span><br><span class="line"><span class="meta">    @score.setter</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self, value</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(value, <span class="built_in">int</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;score must be an integer!&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> value &lt; <span class="number">0</span> <span class="keyword">or</span> value &gt; <span class="number">100</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;score must between 0 ~ 100!&#x27;</span>)</span><br><span class="line">        self._score = value</span><br></pre></td></tr></table></figure>
<p>第一个score方法被<code>@property</code>修改,从而为score属性添加了getter方法,第二个score方法被<code>@score.setter</code>修饰,添加了setter方法.如果去掉第二个score,则score属性将没有setter方法.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lightnine/github.io/Refactor-improving-the-Design-of-Existing-Code.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Refactor-improving-the-Design-of-Existing-Code.html" itemprop="url">Refactor-improving the Design of Existing Code</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-23T15:18:53+08:00">
                2018-06-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">读书笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Refactor-improving the Design of Existing Code(重构-改善既有代码的设计),这是Martin Fowler的一本书.主要是针对Java语言.书已经出版多年了,买这本书也有一年多了,但是一直只看了第一章.最近因为工作不是很忙,就拾起来读了.这里主要是记录下读这本书的一些感想以及收获.<br>读这本书有个收获,重构不单单只是修改函数名称,重命名变量名称那么简单.比如进行类的分解,将合适的函数放置在合适的类中.而这些都需要我们在平常的编程活动中去实践.<br>什么时候进行重构呢?当遇到几个方面时,可以考虑进行重构.发现当为函数添加新功能时,并不能很好的添加进去,这时候我们可以进行重构;当写完功能时,我们也可以考虑重构.总之,重构并不是开发过程中必须要通过的一个过程,就像编译-链接等.你可以随时进行重构.</p>
<h2 id="重构"><a href="#重构" class="headerlink" title="重构"></a>重构</h2><p>我们在平时编程的时候,不可能一次性的就把问题或者程序写的完美无缺,除非我们已经非常熟知某个问题或者某个领域,即使非常熟悉,可能还有我们没有发现的提升之处.那么当我们的程序写的不完美的时候,我们怎么办呢?这时候就可以采用重构的方法来小步快跑的提升我们程序的易读性,优化程序的结构.而这本书给我们提供了如何进行重构的一系列方法.<br>整个开发流程:<br>开发—-&gt; 测试 —-&gt; 重构 —-&gt; 测试</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>在我们重构之前,一定要有合适的测试.在Java中,我们经常用的就是Junit和TestNG了.其中Junit主要是用来做单元测试,而TestNG主要是功能测试或者集成测试.如果我们是开发人员,可能使用Junit比较多,不过TestNG一般也会用到.在我们重构完成之后,一定要跑测试用例,跑通了所有的测试用例,这项重构才算是完成.</p>
<h3 id="代码的坏味道"><a href="#代码的坏味道" class="headerlink" title="代码的坏味道"></a>代码的坏味道</h3><ol>
<li>重复代码<br> 遇到相同的程序结构,应该想办法将它们合二为一,这样程序会变得更好</li>
<li>过长函数<br>函数的代码行数不宜过长,并且函数的命名应该体现出函数做了什么而不是表明怎么做</li>
<li>过大的类<br>当类过大时,我们可以采取Extract class 或者Extract Subclass来减小类的大小</li>
<li>过长的参数列<br>将函数的参数可以用对象来进行传递</li>
<li>发散式变化<br>如果某个类经常因为不同的原因在不同的方向上发生变化,那么这个类就产生了发散式变化.理想情况是针对外界的某一个变化产生的所有相应修改,都应该发生在单一类中.采用 <strong>Extract Class</strong>将因为特定原因造成的所有变化提炼到另一个类中</li>
<li>霰弹式修改<br>如果遇到某种变化,都必须在许多不同的类内做出许多小修改,那么就产生了霰弹式修改.遇到这种情况,应该使用<strong>Move Method</strong>和<strong>Move Field</strong>将所有的修改集中在一个类中.<br>发散式修改是指”一个类受多种变化的影响”,而霰弹式修改是指”一种变化引发多个类的相应修改”.理想情况是外界变化与需要修改的类趋于一一对应.</li>
<li>依恋情结<br>函数对某个类的兴趣高过对自己所处类的兴趣.这时候我们应该将此函数移动到此函数对应的类中.</li>
<li>数据泥团<br>两个类中相同的字段,许多函数签名中相同的参数.这些总是绑定在一起出现的数据应该拥有属于它们的对象</li>
<li>基本类型偏执<br>我们有时候可以用对象来替代基本类型,比如当表示由一个起始值和一个结束值组成的range类,一个币种的money类.我们可以创建对应的小对象.</li>
<li>switch惊悚现身<br>在面向对象程序中尽量少用switch语句,因为switch容易造成重复.而且修改switch条件时,需要找到所有的switch来进行修改</li>
<li>平行继承体系</li>
<li>冗余类<br>消除没有用的类</li>
<li>夸夸其谈未来性</li>
<li>令人迷惑的暂时字段</li>
<li>过度耦合的消息链</li>
<li>中间人<br>不要过度使用委托</li>
<li>狎昵关系</li>
<li>异曲同工的类</li>
<li>不完美的类库</li>
<li>过多的注释</li>
</ol>
<h3 id="重构方法"><a href="#重构方法" class="headerlink" title="重构方法"></a>重构方法</h3><blockquote>
<p>这里仅仅记录了部分的重构方法</p>
</blockquote>
<ol>
<li>提取方法<br>在重构中有一个很核心的动作就是将代码提取为一个单独的方法.这种方式其实也杜绝了重复代码的出现,能够在我们修改代码的时候只需要修改一处就可以.并且能够在多个地方使用</li>
<li>将注释提取为方法<br>理想的程序就是代码完全能够表达自己,当我们在程序中看到注释或者添加注释的时候,我们可以先思考下能否将注释下的代码提取为一个方法,并且方法的函数名称能够体现注释的内容,方法名称要体现程序做了什么而不是怎么做.</li>
<li>明确类的职责<br>类的职责不宜过于多,不宜过于复杂.如果看到一个类承担的职责很多,我们可以考虑是否可以将此类拆分,将不属于其应该承担的责任提取到一个新的类中.然后在源类中通过引用来使用新类中的字段或方法</li>
<li>用查询来代替变量<br>就是在我们使用变量时,我们应该采用计算变量的方法来替代此变量.其实对于这一做法,我是抱有怀疑态度的,因为这造成了函数运行多次.会造成性能下降,但是书中说性能优化属于优化阶段的工作,而且这样重构会为优化阶段带来很好的铺垫.这个我觉得还是主要看平时我们的工作中需要具体情况具体分析.不能尽信书,什么东西都需要我们自己的独立思考</li>
<li>以卫语句取代嵌套条件表达式<br>条件表达式的两种形式:1.所有分支都属于正常分支;2.只有一种是正常行为,其他都是不正常行为.针对情况1,建议使用if…else结构;针对情况2,使用if进行判断,然后直接返回结果.这样子处理能够提高程序清晰度</li>
<li>封装集合<br>当我们在类中有个字段是集合时,我们的返回函数不应该直接返回集合自身,而是应该返回集合的只读副本.另外,不应该为这整个集合提供一个设值函数,应该提供为集合添加,删除元素的函数.</li>
<li>分解条件表达式<br>将复杂的条件表达式分解为较简单的表达式,可是试着尝试将if段落以及else then等提炼为单独的函数</li>
<li>合并条件表达式<br>如果发现检查条件各不相同,但是最终的行为一致.应该使用逻辑与和逻辑或合并为一个条件表示式</li>
<li>引入参数对象<br>可以使用对象来替换函数中的众多参数.比如,遇到数据范围的,参数是一个开始时间,结束时间,那么可以新建一个日期范围类,其中的开始时间和结束时间不能修改.用此类来替换开始时间和结束时间.这样做的好处是可以缩短参数列表.</li>
<li>移除设置函数<br>如果类中的某个字段在对象创建之后不能修改,那么就不提供设置函数,并且将此字段设为<strong>final</strong></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lightnine/github.io/CS231n-Convolutional-Neural-Networks-for-Visual-Recognition.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/CS231n-Convolutional-Neural-Networks-for-Visual-Recognition.html" itemprop="url">CS231n Convolutional Neural Networks for Visual Recognition</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-10T00:42:03+08:00">
                2018-06-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>这是一篇翻译文章,但是也不全是,主要是读了文章之后,用自己的话将其复述出来<br>来源: <a href="https://cs231n.github.io/convolutional-networks/">CS231n Convolutional Neural Networks for Visual Recognition</a></p>
</blockquote>
<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p>卷积神经网络跟一般的神经网络是非常相似的.它们都由神经元组成,并且这些神经元上都有需要学习的权重和偏置项.每个神经元接收输入,执行点积,然后可选择的将输出进行非线性运算.整个网络表示了一个可微的得分函数:即从原始的图像像素到对应的类.两者都有一个损失函数(例如:SVM/Softmax),并且在一般神经网络中用到的技术也能应用到CNN中.<br>两者的区别在哪里呢?ConvNet网络假设输入是图像,所以我们能够将某些属性编码到网络结构中.这使得前馈函数更有效率并且能极大的减少网络中的参数.</p>
<h1 id="CNN结构概览"><a href="#CNN结构概览" class="headerlink" title="CNN结构概览"></a>CNN结构概览</h1><p><em>回顾</em>:在一般的神经网络中,神经网络接收一个输入(一个向量),然后经过一系列的隐藏层进行转换.每个隐藏层是由一组神经元组成,每个神经元都与前一层的所有神经元相连,属于同一层的神经元完全独立并且不共享任何连接.最后一层是输出层,在分类问题中,它代表了类的得分.<br>一般的神经网络不能很好的扩展到完整的图像.在CIFAR-10中,一副图像的大小是32$\times$32$\times$3,所以隐藏层中的一个神经元上将会有32$\times$32$\times$3=3072个权重.参数的数量仍然可以接受,但是全连接结构不能扩展到更大的图像上了.例如,如果图像的大小为200$\times$200$\times$3,那么一个神经元将需要200$\times$200$\times$3=120000个权重.可见,全连接神经网络需要的参数将会非常多.这会导致过拟合.<br>卷积神经网络充分利用了输入是图像的事实,并且用一个更加合理的方式约束了神经网络的结构.特别的,与一般的神经网络不同,卷积神经网络各层由三维排列的神经元组成:宽度,高度,深度(注意,这里的深度指的是激活的第三维,而不是整个神经网络的深度).例如,在CIFAR-10中的输入图像的维度是32$\times$32$\times$3.我们很快会看到,当前层的神经元仅仅连接前一层中的部分神经元.此外,对于CIFAR-10最终的输出层的维度是1$\times$1$\times$10,因为ConvNet最后会将整个图像转换为一个类得分向量,下面是可视化:<br><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column"  style="width: 100%;"><img src="/CS231n-Convolutional-Neural-Networks-for-Visual-Recognition/neural_net2.jpeg" alt="常规神经网络"></div></div><div class="group-picture-row"><div class="group-picture-column"  style="width: 100%;"><img src="/CS231n-Convolutional-Neural-Networks-for-Visual-Recognition/cnn.jpeg" alt="ConvNet"></div></div></div></div><br>上图表示一个三层的神经网络,下图表示卷积神经网络.</p>
<blockquote>
<p>A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.</p>
</blockquote>
<h1 id="ConvNet-层"><a href="#ConvNet-层" class="headerlink" title="ConvNet 层"></a>ConvNet 层</h1><p>就像我们上面描述的,一个简单的ConvNet是一系列层组成,ConvNet的每层通过一个可微的函数将输入转化到输出.ConvNet主要由卷积层,池化层和全连接层组成.下面是一个针对CIFAR-10的简单例子:</p>
<ul>
<li>INPUT(32$\times$32$\times$3):输入是原始图像的像素,一副图像的宽是32,长是32,并且有三个颜色通道R,G,B</li>
<li>卷积层:当前层的神经元只连接前一层的部分神经元.如果我们使用12个过滤器,则经过卷积层后的维度是[32$\times$32$\times$12]</li>
<li>RELU:对于输入采用ReLu激活函数,并不会改变输入维度,如果前一个维度是[32$\times$32$\times$12],则经过ReLu之后仍然是[32$\times$32$\times$12]</li>
<li>POOL:池化层会在空间维度上执行下采样,这会导致维度变化,[16$\times$16$\times$12],但是注意最后一维没有改变</li>
<li><p>FC(全连接层):这是一个全连接的结构,最后的输出是[1$\times$1$\times$10].<br>卷积神经网络就是将原始的像素图像经过一层一层的计算,最后得到最终的分类得分.注意有些层需要参数,而有些层不需要参数.CONV/FC层不仅仅对于输入进行激活,同时需要将权重和偏置作用在输入上.而RELU/POOL只是一个固定的函数,并没有参数.<br>总结:</p>
</li>
<li><p>卷积神经网络就是一系列层组成,将输入图像体积转换为输出体积(体积表面了维度)</p>
</li>
<li>卷积神经网路包括完全不同的层(e.g. CONV/FC/RELU/POOl)</li>
<li>每一层通过一个可微的函数将3D volume的输入转换为3D volume的输出</li>
<li>有些层需要参数,有些不需要(e.g. CONV/FC 需要, RELU/POOlL不需要)</li>
<li>有些层需要额外的超参数,有些不需要(e.g. CONV/FC/POOL需要,RELU不需要)</li>
</ul>
<img src="/CS231n-Convolutional-Neural-Networks-for-Visual-Recognition/convnet.jpeg" class="" title="卷积神经网络结构图">
<p>因为无法很难画出3D的部分,所以这里每一层只展示了深度部分的一片.最后给出了得分最高的五个标签.这里展示的是一个很小的 VGG网络.<a href="http://cs231n.stanford.edu/">查看详细展示</a></p>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>卷积层是卷积神经网络的核心部分,并且涉及了大量的计算.卷积层使用过滤器对于原图像进行卷积,过滤器每次只能针对整副图像的一部分进行计算,所以我们需要移动过滤器,遍历整个图像.这里过滤器的大小又叫做神经元的接收域.<br><em>Example 1</em>:假设输入为[32$\times$32$\times$3],过滤器大小为5$\times$5,那么卷积层中的某个神经元需要的参数为5$\times$5$\times$3 + 1=76.为什么需要这么多的参数呢?针对颜色通道R,当前过滤器对应的局部区域的点是5$\times$5=25个,有三个通道,所有总的参数为75,另外再加一个偏置项,所有一个神经元总共需要76个参数.注意这里的深度为3,这是因为输入的深度是3.</p>
<h2 id="Example-2-假设输入为-16-times-16-times-20-过滤器大小为3-times-3-那么卷积层中每个神经元都需要3-3-20-1-180个参数"><a href="#Example-2-假设输入为-16-times-16-times-20-过滤器大小为3-times-3-那么卷积层中每个神经元都需要3-3-20-1-180个参数" class="headerlink" title="Example 2:假设输入为[16$\times$16$\times$20],过滤器大小为3$\times$3,那么卷积层中每个神经元都需要3*3*20 + 1=180个参数."></a><em>Example 2</em>:假设输入为[16$\times$16$\times$20],过滤器大小为3$\times$3,那么卷积层中每个神经元都需要3*3*20 + 1=180个参数.</h2><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column"  style="width: 100%;"><img src="/CS231n-Convolutional-Neural-Networks-for-Visual-Recognition/depthcol.jpeg" class="" title="卷积层"></div></div><div class="group-picture-row"><div class="group-picture-column"  style="width: 100%;"><img src="/CS231n-Convolutional-Neural-Networks-for-Visual-Recognition/neuron_model.jpeg" class="" title="神经元"></div></div></div></div>
<p>第一张图展示了卷积层,可以看到一个神经元连接了原图像的局部区域,但是连接了所有的深度(这是是三个颜色通道).这里展示了五个神经元,这五个神经元都连接到了图像的同一个局域上.第二张图展示了神经元的计算.<br><strong>Spatial arrangement</strong> 前面我们仅仅讨论了卷积层中的每个神经元如何连接到前一层,我们还没有讨论在卷积层的输出中有多少个神经元.深度,步长,0值填充这些超参数控制着卷积层的输出的大小</p>
<ol>
<li>卷积层输出的深度等于过滤器的数量,而每个过滤器就是去寻找输入到卷积层数据的不同之处.如果输入是原始图像,那么过滤器就是去寻找不同方向的边,颜色等.</li>
<li>步长是过滤器移动的长度,一般常用的是1和2</li>
<li>0值填充就是在卷积层的输入的边界上填充0值,使用0值填充使得我们能够控制经过卷积层之后的空间大小.<br>下面的公式可以用来计算卷积层输出的空间大小<script type="math/tex; mode=display">(W - F + 2P)/S + 1</script>其中W(输入数据的大小),F(卷积层神经元的接收域大小),S(步长),P(零值填充的宽度).例如输入为7$\times$7,过滤器为3$\times$3,步长为1,不填充,则得出输出为5$\times$5.当步长变为2时,那么输出变为3$\times$3.</li>
</ol>
<hr>
<img src="/CS231n-Convolutional-Neural-Networks-for-Visual-Recognition/stride.jpeg" class="" title="空间排列">
<p>这里的输入只有一个x轴,输入为[1,2,-1,1,-3],过滤器大小为3,采用零值填充.所以W=5,F=3,P=1.图片最右侧是过滤器的权重,偏置为0.左图:步长为1,最后得到的输出的尺寸为5;右图:步长为2,最后得到的输出尺寸为3.所有黄色的神经元共享相同的参数</p>
<hr>
<p><em>Use of zero-padding</em>.当步长为1(即S=1),设置$P=(F-1)/2$,这样卷积层的输入跟输出将会有相同大小的空间.<br><em>Constraints on strides</em>.</p>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><h2 id="Normalization-Layer"><a href="#Normalization-Layer" class="headerlink" title="Normalization Layer"></a>Normalization Layer</h2><h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><h2 id="全连接层转变为卷积层"><a href="#全连接层转变为卷积层" class="headerlink" title="全连接层转变为卷积层"></a>全连接层转变为卷积层</h2><h1 id="ConvNet-结构"><a href="#ConvNet-结构" class="headerlink" title="ConvNet 结构"></a>ConvNet 结构</h1><h2 id="层模式"><a href="#层模式" class="headerlink" title="层模式"></a>层模式</h2><h2 id="层大小模式"><a href="#层大小模式" class="headerlink" title="层大小模式"></a>层大小模式</h2><h2 id="常用CNN"><a href="#常用CNN" class="headerlink" title="常用CNN"></a>常用CNN</h2><h2 id="计算考虑"><a href="#计算考虑" class="headerlink" title="计算考虑"></a>计算考虑</h2><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://lightnine/github.io/hexo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2%E5%A4%9A%E8%AE%BE%E5%A4%87%E6%9B%B4%E6%96%B0.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/hexo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2%E5%A4%9A%E8%AE%BE%E5%A4%87%E6%9B%B4%E6%96%B0.html" itemprop="url">hexo搭建github博客多设备更新</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T20:01:32+08:00">
                2018-06-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/hexo/" itemprop="url" rel="index">
                    <span itemprop="name">hexo</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>hexo分支上保存了原始博客文件(相关md文件)，master分支上是博客展示的内容(相关html)</strong></p>
<p>前段时间由于电脑重新安装了系统,导致自己的博客文件丢失,而github上面的只有发布后的文件,没有博客的源文件。想要恢复到源文件至今还没有找到解决方法,好在原来的博客内容不多,丢失了也无所谓了。但是以后要是换电脑了,这种问题怎么解决呢?今天介绍一个解决方案。<br>其实方法很简单,首先应该有两个分支,一个分支用来保存发布后的内容,一个分支用来保存源文件.这里用master分支保存发布后的内容,hexo分支保存源文件内容.</p>
<h2 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a>第一步</h2><p>设备A上搭建github博客</p>
<h2 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a>第二步</h2><p>在博客根目录下,打开_config.yml,添加如下内容</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repository:</span> <span class="string">git@github.com:youname/youname.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以看到现在已经有一个master分支,但是这时候个人博客还不是一个git目录</p>
</blockquote>
<p>提交代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// hexo编译源文件，生成静态文件，也可以分开执行</span><br><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure>
<blockquote>
<p>hexo clean: 清空博客缓存<br>hexo g(hexo generator 的简写):生成静态文件<br>hexo d(hexo deploy的简写): 部署文件,这条命令会使用第一步中的配置信息进行部署</p>
</blockquote>
<p>现在打开yourname.github.io就能够看到你的博客了</p>
<h2 id="第三步"><a href="#第三步" class="headerlink" title="第三步"></a>第三步</h2><p>因为我用的是next主题(其他主题做相似处理).删除next文件下的.git文件夹,这是因为我们在要我们的博客下创建.git,如果子目录下也有.git,会有问题.然后执行以下命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// git初始化</span><br><span class="line">git init</span><br><span class="line">// 新建分支并切换到新建的分支</span><br><span class="line">git checkout -b 分支名</span><br><span class="line">// 添加所有本地文件到git</span><br><span class="line">git add .</span><br><span class="line">// git提交</span><br><span class="line">git commit -m <span class="string">&quot;提交说明&quot;</span></span><br><span class="line">// 文件推送到hexo分支</span><br><span class="line">git push origin hexo</span><br></pre></td></tr></table></figure>
<blockquote>
<p>以后操作都是在hexo分支中,当我们修改了我们的博客内容时,先执行hexo clean &amp;&amp; hexo g &amp;&amp; hexo d ,这个命令用来将本地博客发布到github上<br>然后在将本地内容提交到hexo分支中</p>
</blockquote>
<h2 id="第四步"><a href="#第四步" class="headerlink" title="第四步"></a>第四步</h2><p>假设我们需要在电脑B上搭建我们之前的博客内容.我们需要拉取我们的hexo分支,因为hexo分支是源文件,master可以不用拉取</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 克隆分支到本地</span><br><span class="line">git <span class="built_in">clone</span> -b hexo https://github.com/用户名/仓库名.git</span><br><span class="line">// 进入博客文件夹</span><br><span class="line"><span class="built_in">cd</span> youname.github.io</span><br><span class="line">// 安装依赖</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure>
<h2 id="第五步"><a href="#第五步" class="headerlink" title="第五步"></a>第五步</h2><p>电脑B上编辑博客内容,静态文件提交到master分支,源文件提交到hexo分支</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//博文提交到master上面。</span><br><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</span><br><span class="line">//源文件提交到hexo分支上面。</span><br><span class="line">// 添加源文件</span><br><span class="line">git add .</span><br><span class="line">// git提交</span><br><span class="line">git commit -m <span class="string">&quot;&quot;</span></span><br><span class="line">// 先拉原来Github分支上的源文件到本地，进行合并</span><br><span class="line">git pull origin hexo</span><br><span class="line">// 比较解决前后版本冲突后，push源文件到Github的分支</span><br><span class="line">git push origin hexo</span><br></pre></td></tr></table></figure>
<h2 id="第六步"><a href="#第六步" class="headerlink" title="第六步"></a>第六步</h2><p>在电脑A上可以同步hexo分支,开始更新博客</p>
<p><strong>注意: 以后操作都是在hexo分支中,当我们修改了我们的博客内容时,先执行hexo clean &amp;&amp; hexo g &amp;&amp; hexo d ,这个命令用来将本地博客发布到github上<br>然后在将本地内容提交到hexo分支中</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/">&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">37</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">liang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
